{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Recommender System with SageMaker, MXNet, and Gluon\n",
    "_**Making Video Recommendations Using Neural Networks and Embeddings**_\n",
    "\n",
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "*This work is based on content from the [Cyrus Vahid's 2017 re:Invent Talk](https://github.com/cyrusmvahid/gluontutorials/blob/master/recommendations/MLPMF.ipynb)*\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "  1. [Explore](#Explore)\n",
    "  1. [Clean](#Clean)\n",
    "  1. [Prepare](#Prepare)\n",
    "1. [Train Locally](#Train-Locally)\n",
    "  1. [Define Network](#Define-Network)\n",
    "  1. [Set Parameters](#Set-Parameters)\n",
    "  1. [Execute](#Execute)\n",
    "1. [Train with SageMaker](#Train-with-SageMaker)\n",
    "  1. [Wrap Code](#Wrap-Code)\n",
    "  1. [Move Data](#Move-Data)\n",
    "  1. [Submit](#Submit)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "1. [Wrap-up](#Wrap-up)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.  The minimal required dataset for this is a history of user item ratings.  In our case, we'll use 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "Matrix factorization has been the cornerstone of most user-item prediction models.  This method starts with the large, sparse, user-item ratings in a single matrix, where users index the rows, and items index the columns.  It then seeks to find two lower-dimensional, dense matrices which, when multiplied together, preserve the information and relationships in the larger matrix.\n",
    "\n",
    "![image](https://data-artisans.com/img/blog/factorization.svg)\n",
    "\n",
    "Matrix factorization has been extended and genarlized with deep learning and embeddings.  These techniques allows us to introduce non-linearities for enhanced performance and flexibility.  This notebook will fit a neural network-based model to generate recommendations for the Amazon video dataset.  It will start by exploring our data in the notebook and even training a model on a sample of the data.  Later we'll expand to the full dataset and fit our model using a SageMaker managed training cluster.  We'll then deploy to an endpoint and check our method.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/DEMO-gluon-recsys'\n",
    "\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Python libraries we'll need for the remainder of this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, ndarray\n",
    "from mxnet.metric import MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "### Explore\n",
    "\n",
    "Let's start by bringing in our dataset from an S3 public bucket.  As mentioned above, this contains 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "_Note, because this dataset is over a half gigabyte, the load from S3 may take ~10 minutes.  Also, since Amazon SageMaker Notebooks start with a 5GB persistent volume by default, and we don't need to keep this data on our instance for long, we'll bring it to the temporary volume (which has up to 20GB of storage)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "bucket = 'sagemaker-test-wc'\n",
    "prefix = 'gluon'\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) so that we can begin to understand it.\n",
    "\n",
    "*Note, we'll set `error_bad_lines=False` when reading the file in as there appear to be a very small number of records which would create a problem otherwise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "data_key = 'gluon/train/ratings_sagemaker.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating  timestamp\n",
       "0            1        1193            5  978300760\n",
       "1            1         661            3  978302109\n",
       "2            1         914            3  978301968\n",
       "3            1        3408            4  978300275\n",
       "4            1        2355            5  978824291"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'star_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because most people haven't seen most videos, and people rate fewer videos than we actually watch, we'd expect our data to be sparse.  Our algorithm should work well with this sparse problem in general, but we may still want to clean out some of the long tail.  Let's look at some basic percentiles to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only about 5% of customers have rated 5 or more videos, and only 25% of videos have been rated by 9+ customers.\n",
    "\n",
    "### Clean\n",
    "\n",
    "Let's filter out this long tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll recreate our customer and product lists since there are customers with more than 5 reviews, but all of their reviews are on products with less than 5 reviews (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll number each user and item, giving them their own sequential index.  This will allow us to hold the information in a sparse format where the sequential indices indicate the row and column in our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>4131</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>2377</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>5708</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>1576</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1483</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating  user  item\n",
       "0            1        1193            5  4131    43\n",
       "1            2        1193            5  2377    43\n",
       "2           12        1193            4  5708    43\n",
       "3           15        1193            4  1576    43\n",
       "4           17        1193            5  1483    43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0])})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "Let's start by splitting in training and test sets.  This will allow us to estimate the model's accuracy on videos our customers rated, but wasn't included in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert our Pandas DataFrames into MXNet NDArrays, use those to create a member of the SparseMatrixDataset class, and add that to an MXNet Data Iterator.  This process is the same for both test and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['star_rating'].values, dtype=np.float32))\n",
    "test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['star_rating'].values, dtype=np.float32))\n",
    "\n",
    "train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\n",
    "test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Locally\n",
    "\n",
    "### Define Network\n",
    "\n",
    "Let's start by defining the neural network version of our matrix factorization task.  In this case, our network is quite simple.  The main components are:\n",
    "- [Embeddings](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding) which turn our indexes into dense vectors of fixed size.  In this case, 64.\n",
    "- [Dense layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dense) with ReLU activation.  Each dense layer has the same number of units as our number of embeddings.  Our ReLU activation here also adds some non-linearity to our matrix factorization.\n",
    "- [Dropout layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dropout) which can be used to prevent over-fitting.\n",
    "- Matrix multiplication of our user matrix and our item matrix to create an estimate of our rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFBlock(gluon.HybridBlock):\n",
    "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\n",
    "        super(MFBlock, self).__init__()\n",
    "        \n",
    "        self.max_users = max_users\n",
    "        self.max_items = max_items\n",
    "        self.dropout_p = dropout_p\n",
    "        self.num_emb = num_emb\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\n",
    "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\n",
    "            \n",
    "            self.dropout_user = gluon.nn.Dropout(dropout_p)\n",
    "            self.dropout_item = gluon.nn.Dropout(dropout_p)\n",
    "\n",
    "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            \n",
    "    def hybrid_forward(self, F, users, items):\n",
    "        a = self.user_embeddings(users)\n",
    "        a = self.dense_user(a)\n",
    "        \n",
    "        b = self.item_embeddings(items)\n",
    "        b = self.dense_item(b)\n",
    "\n",
    "        predictions = self.dropout_user(a) * self.dropout_item(b)     \n",
    "        predictions = F.sum(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 64\n",
    "\n",
    "net = MFBlock(max_users=customer_index.shape[0], \n",
    "              max_items=product_index.shape[0],\n",
    "              num_emb=num_embeddings,\n",
    "              dropout_p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters\n",
    "\n",
    "Let's initialize network weights and set our optimization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters\n",
    "ctx = mx.cpu()\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=60),\n",
    "                                ctx=ctx,\n",
    "                                force_reinit=True)\n",
    "net.hybridize()\n",
    "\n",
    "# Set optimization parameters\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0.\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n",
    "\n",
    "Let's define a function to carry out the training of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(train_iter, test_iter, net, epochs, ctx):\n",
    "    \n",
    "    loss_function = gluon.loss.L2Loss()\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        print(\"epoch: {}\".format(e))\n",
    "        \n",
    "        for i, (user, item, label) in enumerate(train_iter):\n",
    "                user = user.as_in_context(ctx)\n",
    "                item = item.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                \n",
    "                with mx.autograd.record():\n",
    "                    output = net(user, item)               \n",
    "                    loss = loss_function(output, label)\n",
    "                    \n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "\n",
    "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\n",
    "                                                                   eval_net(train_iter, net, ctx, loss_function),\n",
    "                                                                   eval_net(test_iter, net, ctx, loss_function)))\n",
    "    print(\"end of training\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function which evaluates our network on a given dataset.  This is called by our `execute` function above to provide mean squared error values on our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(data, net, ctx, loss_function):\n",
    "    acc = MSE()\n",
    "    for i, (user, item, label) in enumerate(data):\n",
    "        \n",
    "            user = user.as_in_context(ctx)\n",
    "            item = item.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            predictions = net(user, item).reshape((batch_size, 1))\n",
    "            acc.update(preds=[predictions], labels=[label])\n",
    "   \n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 1.1989251252428756. 1.1990698955722692\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 1.0789494939248176. 1.078927003269215\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 0.996950554614455. 0.9970031707834387\n",
      "end of training\n",
      "CPU times: user 3min 45s, sys: 3.63 s, total: 3min 48s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Validation\n",
    "\n",
    "We can see our training error going down, but our validation accuracy bounces around a bit.  Let's check how our model is predicting for an individual user.  We could pick randomly, but for this case, let's try user #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>product_id</th>\n",
       "      <th>u6_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>3613</td>\n",
       "      <td>3647</td>\n",
       "      <td>4.534433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>3548</td>\n",
       "      <td>1905</td>\n",
       "      <td>4.328877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>3360</td>\n",
       "      <td>1532</td>\n",
       "      <td>4.322484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>3699</td>\n",
       "      <td>1714</td>\n",
       "      <td>4.199345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>3526</td>\n",
       "      <td>572</td>\n",
       "      <td>4.171098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>3519</td>\n",
       "      <td>827</td>\n",
       "      <td>4.131466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>3099</td>\n",
       "      <td>197</td>\n",
       "      <td>4.129376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>3484</td>\n",
       "      <td>744</td>\n",
       "      <td>4.061161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466</td>\n",
       "      <td>3522</td>\n",
       "      <td>4.032640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>3623</td>\n",
       "      <td>658</td>\n",
       "      <td>4.030008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>3590</td>\n",
       "      <td>134</td>\n",
       "      <td>4.010974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3695</th>\n",
       "      <td>3695</td>\n",
       "      <td>2742</td>\n",
       "      <td>4.002236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>3452</td>\n",
       "      <td>1877</td>\n",
       "      <td>3.977762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>3448</td>\n",
       "      <td>2127</td>\n",
       "      <td>3.974182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>3449</td>\n",
       "      <td>3314</td>\n",
       "      <td>3.967001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>3614</td>\n",
       "      <td>2845</td>\n",
       "      <td>3.954757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>3454</td>\n",
       "      <td>2299</td>\n",
       "      <td>3.911975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2762</td>\n",
       "      <td>3.908178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>3681</td>\n",
       "      <td>826</td>\n",
       "      <td>3.884180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>3379</td>\n",
       "      <td>1901</td>\n",
       "      <td>3.884170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>3588</td>\n",
       "      <td>884</td>\n",
       "      <td>3.865048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>3261</td>\n",
       "      <td>3636</td>\n",
       "      <td>3.855571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>3543</td>\n",
       "      <td>887</td>\n",
       "      <td>3.834744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>3536</td>\n",
       "      <td>3228</td>\n",
       "      <td>3.833851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>3416</td>\n",
       "      <td>3205</td>\n",
       "      <td>3.830016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>1128</td>\n",
       "      <td>3429</td>\n",
       "      <td>3.817164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>3370</td>\n",
       "      <td>3292</td>\n",
       "      <td>3.808532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>3111</td>\n",
       "      <td>3050</td>\n",
       "      <td>3.805593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>1252</td>\n",
       "      <td>3.804578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>3377</td>\n",
       "      <td>2444</td>\n",
       "      <td>3.786094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>1903</td>\n",
       "      <td>2514</td>\n",
       "      <td>1.519426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>1383</td>\n",
       "      <td>1389</td>\n",
       "      <td>1.519011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>2633</td>\n",
       "      <td>1105</td>\n",
       "      <td>1.511741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2020</td>\n",
       "      <td>1981</td>\n",
       "      <td>1.510844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>1661</td>\n",
       "      <td>502</td>\n",
       "      <td>1.505280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>531</td>\n",
       "      <td>160</td>\n",
       "      <td>1.502507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>1920</td>\n",
       "      <td>169</td>\n",
       "      <td>1.502080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1343</td>\n",
       "      <td>193</td>\n",
       "      <td>1.500691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>2294</td>\n",
       "      <td>1599</td>\n",
       "      <td>1.498610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>1771</td>\n",
       "      <td>3054</td>\n",
       "      <td>1.491658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>1552</td>\n",
       "      <td>2381</td>\n",
       "      <td>1.481367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>2237</td>\n",
       "      <td>3041</td>\n",
       "      <td>1.479133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>967</td>\n",
       "      <td>2643</td>\n",
       "      <td>1.455463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>2560</td>\n",
       "      <td>1739</td>\n",
       "      <td>1.454525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>1682</td>\n",
       "      <td>2382</td>\n",
       "      <td>1.452837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2389</td>\n",
       "      <td>102</td>\n",
       "      <td>1.452068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>874</td>\n",
       "      <td>1556</td>\n",
       "      <td>1.440914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>1684</td>\n",
       "      <td>2383</td>\n",
       "      <td>1.420558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>1510</td>\n",
       "      <td>3877</td>\n",
       "      <td>1.419270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>1929</td>\n",
       "      <td>1996</td>\n",
       "      <td>1.417286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>2021</td>\n",
       "      <td>2799</td>\n",
       "      <td>1.415281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>2074</td>\n",
       "      <td>181</td>\n",
       "      <td>1.411192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>926</td>\n",
       "      <td>546</td>\n",
       "      <td>1.399087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>2219</td>\n",
       "      <td>631</td>\n",
       "      <td>1.397912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>1527</td>\n",
       "      <td>3268</td>\n",
       "      <td>1.381755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>1875</td>\n",
       "      <td>810</td>\n",
       "      <td>1.374361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>1041</td>\n",
       "      <td>1381</td>\n",
       "      <td>1.369832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>1607</td>\n",
       "      <td>2555</td>\n",
       "      <td>1.351865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>1848</td>\n",
       "      <td>2817</td>\n",
       "      <td>1.284432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>951</td>\n",
       "      <td>3593</td>\n",
       "      <td>1.083398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3706 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item  product_id  u6_predictions\n",
       "3613  3613        3647        4.534433\n",
       "3548  3548        1905        4.328877\n",
       "3360  3360        1532        4.322484\n",
       "3699  3699        1714        4.199345\n",
       "3526  3526         572        4.171098\n",
       "3519  3519         827        4.131466\n",
       "3099  3099         197        4.129376\n",
       "3484  3484         744        4.061161\n",
       "3466  3466        3522        4.032640\n",
       "3623  3623         658        4.030008\n",
       "3590  3590         134        4.010974\n",
       "3695  3695        2742        4.002236\n",
       "3452  3452        1877        3.977762\n",
       "3448  3448        2127        3.974182\n",
       "3449  3449        3314        3.967001\n",
       "3614  3614        2845        3.954757\n",
       "3454  3454        2299        3.911975\n",
       "13      13        2762        3.908178\n",
       "3681  3681         826        3.884180\n",
       "3379  3379        1901        3.884170\n",
       "3588  3588         884        3.865048\n",
       "3261  3261        3636        3.855571\n",
       "3543  3543         887        3.834744\n",
       "3536  3536        3228        3.833851\n",
       "3416  3416        3205        3.830016\n",
       "1128  1128        3429        3.817164\n",
       "3370  3370        3292        3.808532\n",
       "3111  3111        3050        3.805593\n",
       "133    133        1252        3.804578\n",
       "3377  3377        2444        3.786094\n",
       "...    ...         ...             ...\n",
       "1903  1903        2514        1.519426\n",
       "1383  1383        1389        1.519011\n",
       "2633  2633        1105        1.511741\n",
       "2020  2020        1981        1.510844\n",
       "1661  1661         502        1.505280\n",
       "531    531         160        1.502507\n",
       "1920  1920         169        1.502080\n",
       "1343  1343         193        1.500691\n",
       "2294  2294        1599        1.498610\n",
       "1771  1771        3054        1.491658\n",
       "1552  1552        2381        1.481367\n",
       "2237  2237        3041        1.479133\n",
       "967    967        2643        1.455463\n",
       "2560  2560        1739        1.454525\n",
       "1682  1682        2382        1.452837\n",
       "2389  2389         102        1.452068\n",
       "874    874        1556        1.440914\n",
       "1684  1684        2383        1.420558\n",
       "1510  1510        3877        1.419270\n",
       "1929  1929        1996        1.417286\n",
       "2021  2021        2799        1.415281\n",
       "2074  2074         181        1.411192\n",
       "926    926         546        1.399087\n",
       "2219  2219         631        1.397912\n",
       "1527  1527        3268        1.381755\n",
       "1875  1875         810        1.374361\n",
       "1041  1041        1381        1.369832\n",
       "1607  1607        2555        1.351865\n",
       "1848  1848        2817        1.284432\n",
       "951    951        3593        1.083398\n",
       "\n",
       "[3706 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_index['u6_predictions'] = trained_net(nd.array([6] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u6_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this to the predictions for another user (we'll try user #7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>product_id</th>\n",
       "      <th>u6_predictions</th>\n",
       "      <th>u7_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>3548</td>\n",
       "      <td>1905</td>\n",
       "      <td>4.328877</td>\n",
       "      <td>4.675824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>3613</td>\n",
       "      <td>3647</td>\n",
       "      <td>4.534433</td>\n",
       "      <td>4.503952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>3543</td>\n",
       "      <td>887</td>\n",
       "      <td>3.834744</td>\n",
       "      <td>4.489244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>3526</td>\n",
       "      <td>572</td>\n",
       "      <td>4.171098</td>\n",
       "      <td>4.469326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466</td>\n",
       "      <td>3522</td>\n",
       "      <td>4.032640</td>\n",
       "      <td>4.422765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>3699</td>\n",
       "      <td>1714</td>\n",
       "      <td>4.199345</td>\n",
       "      <td>4.302065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>3452</td>\n",
       "      <td>1877</td>\n",
       "      <td>3.977762</td>\n",
       "      <td>4.295789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>3449</td>\n",
       "      <td>3314</td>\n",
       "      <td>3.967001</td>\n",
       "      <td>4.262650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>3416</td>\n",
       "      <td>3205</td>\n",
       "      <td>3.830016</td>\n",
       "      <td>4.211958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>3360</td>\n",
       "      <td>1532</td>\n",
       "      <td>4.322484</td>\n",
       "      <td>4.187609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>3448</td>\n",
       "      <td>2127</td>\n",
       "      <td>3.974182</td>\n",
       "      <td>4.166749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>3454</td>\n",
       "      <td>2299</td>\n",
       "      <td>3.911975</td>\n",
       "      <td>4.131511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3695</th>\n",
       "      <td>3695</td>\n",
       "      <td>2742</td>\n",
       "      <td>4.002236</td>\n",
       "      <td>4.111141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>3536</td>\n",
       "      <td>3228</td>\n",
       "      <td>3.833851</td>\n",
       "      <td>4.108009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>3432</td>\n",
       "      <td>3609</td>\n",
       "      <td>3.524951</td>\n",
       "      <td>4.102628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>3484</td>\n",
       "      <td>744</td>\n",
       "      <td>4.061161</td>\n",
       "      <td>4.082273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>3390</td>\n",
       "      <td>1098</td>\n",
       "      <td>3.750699</td>\n",
       "      <td>4.073951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>3590</td>\n",
       "      <td>134</td>\n",
       "      <td>4.010974</td>\n",
       "      <td>4.073165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>3623</td>\n",
       "      <td>658</td>\n",
       "      <td>4.030008</td>\n",
       "      <td>4.070432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>3519</td>\n",
       "      <td>827</td>\n",
       "      <td>4.131466</td>\n",
       "      <td>4.060688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>749</td>\n",
       "      <td>916</td>\n",
       "      <td>3.560303</td>\n",
       "      <td>4.050517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2762</td>\n",
       "      <td>3.908178</td>\n",
       "      <td>4.025032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>3261</td>\n",
       "      <td>3636</td>\n",
       "      <td>3.855571</td>\n",
       "      <td>4.021749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>1252</td>\n",
       "      <td>3.804578</td>\n",
       "      <td>4.020889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>1143</td>\n",
       "      <td>3307</td>\n",
       "      <td>3.742656</td>\n",
       "      <td>4.014178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>3314</td>\n",
       "      <td>2063</td>\n",
       "      <td>3.477116</td>\n",
       "      <td>4.006401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>3435</td>\n",
       "      <td>3.711648</td>\n",
       "      <td>3.999594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3554</th>\n",
       "      <td>3554</td>\n",
       "      <td>706</td>\n",
       "      <td>3.618642</td>\n",
       "      <td>3.961318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>3588</td>\n",
       "      <td>884</td>\n",
       "      <td>3.865048</td>\n",
       "      <td>3.955656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>3652</td>\n",
       "      <td>730</td>\n",
       "      <td>3.676623</td>\n",
       "      <td>3.948435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>3393</td>\n",
       "      <td>3232</td>\n",
       "      <td>1.910734</td>\n",
       "      <td>1.656738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2010</td>\n",
       "      <td>3799</td>\n",
       "      <td>1.615762</td>\n",
       "      <td>1.655034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1468</td>\n",
       "      <td>1760</td>\n",
       "      <td>1.548045</td>\n",
       "      <td>1.650585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>1920</td>\n",
       "      <td>169</td>\n",
       "      <td>1.502080</td>\n",
       "      <td>1.641755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>1210</td>\n",
       "      <td>3439</td>\n",
       "      <td>1.576959</td>\n",
       "      <td>1.634530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>2021</td>\n",
       "      <td>2799</td>\n",
       "      <td>1.415281</td>\n",
       "      <td>1.629275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>2299</td>\n",
       "      <td>2974</td>\n",
       "      <td>1.596935</td>\n",
       "      <td>1.627997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>1771</td>\n",
       "      <td>3054</td>\n",
       "      <td>1.491658</td>\n",
       "      <td>1.622503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2020</td>\n",
       "      <td>1981</td>\n",
       "      <td>1.510844</td>\n",
       "      <td>1.610587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2389</td>\n",
       "      <td>102</td>\n",
       "      <td>1.452068</td>\n",
       "      <td>1.608018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>2331</td>\n",
       "      <td>1731</td>\n",
       "      <td>1.532919</td>\n",
       "      <td>1.605468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>2612</td>\n",
       "      <td>1323</td>\n",
       "      <td>1.569208</td>\n",
       "      <td>1.591380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1343</td>\n",
       "      <td>193</td>\n",
       "      <td>1.500691</td>\n",
       "      <td>1.587853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>926</td>\n",
       "      <td>546</td>\n",
       "      <td>1.399087</td>\n",
       "      <td>1.571807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>967</td>\n",
       "      <td>2643</td>\n",
       "      <td>1.455463</td>\n",
       "      <td>1.560284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>1903</td>\n",
       "      <td>2514</td>\n",
       "      <td>1.519426</td>\n",
       "      <td>1.553169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>1848</td>\n",
       "      <td>2817</td>\n",
       "      <td>1.284432</td>\n",
       "      <td>1.534215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>1041</td>\n",
       "      <td>1381</td>\n",
       "      <td>1.369832</td>\n",
       "      <td>1.529024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>1607</td>\n",
       "      <td>2555</td>\n",
       "      <td>1.351865</td>\n",
       "      <td>1.513492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>2036</td>\n",
       "      <td>66</td>\n",
       "      <td>1.673373</td>\n",
       "      <td>1.478088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>1912</td>\n",
       "      <td>1998</td>\n",
       "      <td>1.560990</td>\n",
       "      <td>1.474279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>874</td>\n",
       "      <td>1556</td>\n",
       "      <td>1.440914</td>\n",
       "      <td>1.463312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>1875</td>\n",
       "      <td>810</td>\n",
       "      <td>1.374361</td>\n",
       "      <td>1.460744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>1929</td>\n",
       "      <td>1996</td>\n",
       "      <td>1.417286</td>\n",
       "      <td>1.438443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>2074</td>\n",
       "      <td>181</td>\n",
       "      <td>1.411192</td>\n",
       "      <td>1.433488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>1527</td>\n",
       "      <td>3268</td>\n",
       "      <td>1.381755</td>\n",
       "      <td>1.433301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>1684</td>\n",
       "      <td>2383</td>\n",
       "      <td>1.420558</td>\n",
       "      <td>1.420691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>2633</td>\n",
       "      <td>1105</td>\n",
       "      <td>1.511741</td>\n",
       "      <td>1.415294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>2560</td>\n",
       "      <td>1739</td>\n",
       "      <td>1.454525</td>\n",
       "      <td>1.412890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>951</td>\n",
       "      <td>3593</td>\n",
       "      <td>1.083398</td>\n",
       "      <td>1.184552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3706 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      item  product_id  u6_predictions  u7_predictions\n",
       "3548  3548        1905        4.328877        4.675824\n",
       "3613  3613        3647        4.534433        4.503952\n",
       "3543  3543         887        3.834744        4.489244\n",
       "3526  3526         572        4.171098        4.469326\n",
       "3466  3466        3522        4.032640        4.422765\n",
       "3699  3699        1714        4.199345        4.302065\n",
       "3452  3452        1877        3.977762        4.295789\n",
       "3449  3449        3314        3.967001        4.262650\n",
       "3416  3416        3205        3.830016        4.211958\n",
       "3360  3360        1532        4.322484        4.187609\n",
       "3448  3448        2127        3.974182        4.166749\n",
       "3454  3454        2299        3.911975        4.131511\n",
       "3695  3695        2742        4.002236        4.111141\n",
       "3536  3536        3228        3.833851        4.108009\n",
       "3432  3432        3609        3.524951        4.102628\n",
       "3484  3484         744        4.061161        4.082273\n",
       "3390  3390        1098        3.750699        4.073951\n",
       "3590  3590         134        4.010974        4.073165\n",
       "3623  3623         658        4.030008        4.070432\n",
       "3519  3519         827        4.131466        4.060688\n",
       "749    749         916        3.560303        4.050517\n",
       "13      13        2762        3.908178        4.025032\n",
       "3261  3261        3636        3.855571        4.021749\n",
       "133    133        1252        3.804578        4.020889\n",
       "1143  1143        3307        3.742656        4.014178\n",
       "3314  3314        2063        3.477116        4.006401\n",
       "557    557        3435        3.711648        3.999594\n",
       "3554  3554         706        3.618642        3.961318\n",
       "3588  3588         884        3.865048        3.955656\n",
       "3652  3652         730        3.676623        3.948435\n",
       "...    ...         ...             ...             ...\n",
       "3393  3393        3232        1.910734        1.656738\n",
       "2010  2010        3799        1.615762        1.655034\n",
       "1468  1468        1760        1.548045        1.650585\n",
       "1920  1920         169        1.502080        1.641755\n",
       "1210  1210        3439        1.576959        1.634530\n",
       "2021  2021        2799        1.415281        1.629275\n",
       "2299  2299        2974        1.596935        1.627997\n",
       "1771  1771        3054        1.491658        1.622503\n",
       "2020  2020        1981        1.510844        1.610587\n",
       "2389  2389         102        1.452068        1.608018\n",
       "2331  2331        1731        1.532919        1.605468\n",
       "2612  2612        1323        1.569208        1.591380\n",
       "1343  1343         193        1.500691        1.587853\n",
       "926    926         546        1.399087        1.571807\n",
       "967    967        2643        1.455463        1.560284\n",
       "1903  1903        2514        1.519426        1.553169\n",
       "1848  1848        2817        1.284432        1.534215\n",
       "1041  1041        1381        1.369832        1.529024\n",
       "1607  1607        2555        1.351865        1.513492\n",
       "2036  2036          66        1.673373        1.478088\n",
       "1912  1912        1998        1.560990        1.474279\n",
       "874    874        1556        1.440914        1.463312\n",
       "1875  1875         810        1.374361        1.460744\n",
       "1929  1929        1996        1.417286        1.438443\n",
       "2074  2074         181        1.411192        1.433488\n",
       "1527  1527        3268        1.381755        1.433301\n",
       "1684  1684        2383        1.420558        1.420691\n",
       "2633  2633        1105        1.511741        1.415294\n",
       "2560  2560        1739        1.454525        1.412890\n",
       "951    951        3593        1.083398        1.184552\n",
       "\n",
       "[3706 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_index['u7_predictions'] = trained_net(nd.array([7] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u7_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted ratings are different between the two users, but the same top (and bottom) items for user #6 appear for #7 as well.  Let's look at the correlation across the full set of 38K items to see if this relationship holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "product_index[['u6_predictions', 'u7_predictions']].plot.scatter('u6_predictions', 'u7_predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this correlation is nearly perfect.  Essentially the average rating of items dominates across users and we'll recommend the same well-reviewed items to everyone.  As it turns out, we can add more embeddings and this relationship will go away since we're better able to capture differential preferences across users.\n",
    "\n",
    "However, with just a 64 dimensional embedding, it took 7 minutes to run just 3 epochs.  If we ran this outside of our Notebook Instance we could run larger jobs and move on to other work would improve productivity.\n",
    "\n",
    "---\n",
    "\n",
    "## Train with SageMaker\n",
    "\n",
    "Now that we've trained on this smaller dataset, we can expand training in SageMaker's distributed, managed training environment.\n",
    "\n",
    "### Wrap Code\n",
    "\n",
    "To use SageMaker's pre-built MXNet container, we'll need to wrap our code from above into a Python script.  There's a great deal of flexibility in using SageMaker's pre-built containers, and detailed documentation can be found [here](https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators), but for our example, it consisted of:\n",
    "1. Wrapping all data preparation into a `prepare_train_data` function (we could name this whatever we like)\n",
    "1. Copying and pasting classes and functions from above word-for-word\n",
    "1. Defining a `train` function that:\n",
    "  1. Adds a bit of new code to pick up the input TSV dataset on the SageMaker Training cluster\n",
    "  1. Takes in a dict of hyperparameters (which we specified as globals above)\n",
    "  1. Creates the net and executes training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\r\n",
      "import json\r\n",
      "import time\r\n",
      "import os\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, nd, ndarray\r\n",
      "from mxnet.metric import MSE\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "os.system('pip install pandas')\r\n",
      "import pandas as pd\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "#########\r\n",
      "# Globals\r\n",
      "#########\r\n",
      "\r\n",
      "batch_size = 1024\r\n",
      "\r\n",
      "\r\n",
      "##########\r\n",
      "# Training\r\n",
      "##########\r\n",
      "\r\n",
      "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, **kwargs):\r\n",
      "    \r\n",
      "    # get data\r\n",
      "    training_dir = channel_input_dirs['train']\r\n",
      "    train_iter, test_iter, customer_index, product_index = prepare_train_data(training_dir)\r\n",
      "    \r\n",
      "    # get hyperparameters\r\n",
      "    num_embeddings = hyperparameters.get('num_embeddings', 64)\r\n",
      "    opt = hyperparameters.get('opt', 'sgd')\r\n",
      "    lr = hyperparameters.get('lr', 0.02)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    wd = hyperparameters.get('wd', 0.)\r\n",
      "    epochs = hyperparameters.get('epochs', 5)\r\n",
      "\r\n",
      "    # define net\r\n",
      "    ctx = mx.cpu()\r\n",
      "\r\n",
      "    net = MFBlock(max_users=customer_index.shape[0], \r\n",
      "                  max_items=product_index.shape[0],\r\n",
      "                  num_emb=num_embeddings,\r\n",
      "                  dropout_p=0.5)\r\n",
      "    \r\n",
      "    net.collect_params().initialize(mx.init.Xavier(magnitude=60),\r\n",
      "                                    ctx=ctx,\r\n",
      "                                    force_reinit=True)\r\n",
      "    net.hybridize()\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(),\r\n",
      "                            opt,\r\n",
      "                            {'learning_rate': lr,\r\n",
      "                             'wd': wd,\r\n",
      "                             'momentum': momentum})\r\n",
      "    \r\n",
      "    # execute\r\n",
      "    trained_net = execute(train_iter, test_iter, net, trainer, epochs, ctx)\r\n",
      "    \r\n",
      "    return trained_net, customer_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "class MFBlock(gluon.HybridBlock):\r\n",
      "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\r\n",
      "        super(MFBlock, self).__init__()\r\n",
      "        \r\n",
      "        self.max_users = max_users\r\n",
      "        self.max_items = max_items\r\n",
      "        self.dropout_p = dropout_p\r\n",
      "        self.num_emb = num_emb\r\n",
      "        \r\n",
      "        with self.name_scope():\r\n",
      "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\r\n",
      "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\r\n",
      "\r\n",
      "            self.dropout_user = gluon.nn.Dropout(dropout_p)\r\n",
      "            self.dropout_item = gluon.nn.Dropout(dropout_p)\r\n",
      "\r\n",
      "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\r\n",
      "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\r\n",
      "            \r\n",
      "    def hybrid_forward(self, F, users, items):\r\n",
      "        a = self.user_embeddings(users)\r\n",
      "        a = self.dense_user(a)\r\n",
      "        \r\n",
      "        b = self.item_embeddings(items)\r\n",
      "        b = self.dense_item(b)\r\n",
      "\r\n",
      "        predictions = self.dropout_user(a) * self.dropout_item(b)      \r\n",
      "        predictions = F.sum(predictions, axis=1)\r\n",
      "\r\n",
      "        return predictions\r\n",
      "\r\n",
      "    \r\n",
      "def execute(train_iter, test_iter, net, trainer, epochs, ctx):\r\n",
      "    loss_function = gluon.loss.L2Loss()\r\n",
      "    for e in range(epochs):\r\n",
      "        print(\"epoch: {}\".format(e))\r\n",
      "        for i, (user, item, label) in enumerate(train_iter):\r\n",
      "\r\n",
      "                user = user.as_in_context(ctx)\r\n",
      "                item = item.as_in_context(ctx)\r\n",
      "                label = label.as_in_context(ctx)\r\n",
      "\r\n",
      "                with mx.autograd.record():\r\n",
      "                    output = net(user, item)               \r\n",
      "                    loss = loss_function(output, label)\r\n",
      "                loss.backward()\r\n",
      "                trainer.step(batch_size)\r\n",
      "\r\n",
      "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\r\n",
      "                                                                   eval_net(train_iter, net, ctx, loss_function),\r\n",
      "                                                                   eval_net(test_iter, net, ctx, loss_function)))\r\n",
      "    print(\"end of training\")\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def eval_net(data, net, ctx, loss_function):\r\n",
      "    acc = MSE()\r\n",
      "    for i, (user, item, label) in enumerate(data):\r\n",
      "\r\n",
      "            user = user.as_in_context(ctx)\r\n",
      "            item = item.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "\r\n",
      "            predictions = net(user, item).reshape((batch_size, 1))\r\n",
      "            acc.update(preds=[predictions], labels=[label])\r\n",
      "\r\n",
      "    return acc.get()[1]\r\n",
      "\r\n",
      "\r\n",
      "def save(model, model_dir):\r\n",
      "    net, customer_index, product_index = model\r\n",
      "    net.save_params('{}/model.params'.format(model_dir))\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'w')\r\n",
      "    json.dump({'max_users': net.max_users,\r\n",
      "               'max_items': net.max_items,\r\n",
      "               'num_emb': net.num_emb,\r\n",
      "               'dropout_p': net.dropout_p},\r\n",
      "              f)\r\n",
      "    f.close()\r\n",
      "    customer_index.to_csv('{}/customer_index.csv'.format(model_dir), index=False)\r\n",
      "    product_index.to_csv('{}/product_index.csv'.format(model_dir), index=False)\r\n",
      "\r\n",
      "    \r\n",
      "######\r\n",
      "# Data\r\n",
      "######\r\n",
      "\r\n",
      "def prepare_train_data(training_dir):\r\n",
      "    f = os.listdir(training_dir)\r\n",
      "    df = pd.read_csv(os.path.join(training_dir, f[0]))\r\n",
      "    df = df[['customer_id', 'product_id', 'star_rating']]\r\n",
      "    customers = df['customer_id'].value_counts()\r\n",
      "    products = df['product_id'].value_counts()\r\n",
      "    \r\n",
      "    reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))\r\n",
      "    customers = reduced_df['customer_id'].value_counts()\r\n",
      "    products = reduced_df['product_id'].value_counts()\r\n",
      "\r\n",
      "    # Number users and items\r\n",
      "    customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\r\n",
      "    product_index = pd.DataFrame({'product_id': products.index, 'item': np.arange(products.shape[0])})\r\n",
      "\r\n",
      "    reduced_df = reduced_df.merge(customer_index).merge(product_index)\r\n",
      "\r\n",
      "    # Split train and test\r\n",
      "    test_df = reduced_df.groupby('customer_id').last().reset_index()\r\n",
      "\r\n",
      "    train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \r\n",
      "                                on=['customer_id', 'product_id'], \r\n",
      "                                how='outer', \r\n",
      "                                indicator=True)\r\n",
      "    train_df = train_df[(train_df['_merge'] == 'left_only')]\r\n",
      "\r\n",
      "    # MXNet data iterators\r\n",
      "    train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32), \r\n",
      "                                    nd.array(train_df['item'].values, dtype=np.float32),\r\n",
      "                                    nd.array(train_df['star_rating'].values, dtype=np.float32))\r\n",
      "    test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32), \r\n",
      "                                    nd.array(test_df['item'].values, dtype=np.float32),\r\n",
      "                                    nd.array(test_df['star_rating'].values, dtype=np.float32))\r\n",
      "\r\n",
      "    train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\r\n",
      "    test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\r\n",
      "\r\n",
      "    return train_iter, test_iter, customer_index, product_index \r\n",
      "\r\n",
      "#########\r\n",
      "# Hosting\r\n",
      "#########\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'r')\r\n",
      "    block_params = json.load(f)\r\n",
      "    f.close()\r\n",
      "    net = MFBlock(max_users=block_params['max_users'], \r\n",
      "                  max_items=block_params['max_items'],\r\n",
      "                  num_emb=block_params['num_emb'],\r\n",
      "                  dropout_p=block_params['dropout_p'])\r\n",
      "    net.load_params('{}/model.params'.format(model_dir), ctx)\r\n",
      "    customer_index = pd.read_csv('{}/customer_index.csv'.format(model_dir))\r\n",
      "    product_index = pd.read_csv('{}/product_index.csv'.format(model_dir))\r\n",
      "    return net, customer_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    parsed = json.loads(data)\r\n",
      "\r\n",
      "    trained_net, customer_index, product_index = net\r\n",
      "    users = pd.DataFrame({'customer_id': parsed['customer_id']}).merge(customer_index, how='left')['user'].values\r\n",
      "    items = pd.DataFrame({'product_id': parsed['product_id']}).merge(product_index, how='left')['item'].values\r\n",
      "    \r\n",
      "    predictions = trained_net(nd.array(users).as_in_context(ctx), nd.array(items).as_in_context(ctx))\r\n",
      "    response_body = json.dumps(predictions.asnumpy().tolist())\r\n",
      "\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat recommender_test_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Locally\n",
    "\n",
    "Now we can test our train function locally.  This helps ensure we don't have any bugs before submitting our code to SageMaker's pre-built MXNet container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import recommender\n",
    "\n",
    "# local_test_net, local_customer_index, local_product_index = recommender.train(\n",
    "#     {'train': '/tmp/recsys/'}, \n",
    "#     {'num_embeddings': 64, \n",
    "#      'opt': 'sgd', \n",
    "#      'lr': 0.02, \n",
    "#      'momentum': 0.9, \n",
    "#      'wd': 0.,\n",
    "#      'epochs': 3},\n",
    "#     ['local'],\n",
    "#     1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Data\n",
    "\n",
    "Holding our data in memory works fine when we're interactively exploring a sample of data, but for larger, longer running processes, we'd prfer to run them in the background with SageMaker Training.  To do this, let's move the dataset to S3 so that it can be picked up by SageMaker training.  This is perfect for use cases like periodic re-training, expanding to a larger dataset, or moving production workloads to larger hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boto3.client('s3').copy({'Bucket': 'amazon-reviews-pds', \n",
    "#                         'Key': 'tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz'},\n",
    "#                        bucket,\n",
    "#                        prefix + '/train/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit\n",
    "\n",
    "Now, we can create an MXNet estimator from the SageMaker Python SDK.  To do so, we need to pass in:\n",
    "1. Instance type and count for our SageMaker Training cluster.  SageMaker's MXNet containers support distributed GPU training, so we could easily set this to multiple ml.p2 or ml.p3 instances if we wanted.\n",
    "  - *Note, this would require some changes to our recommender.py script as we would need to setup the context an key value store properly, as well as determining if and how to distribute the training data.*\n",
    "1. An S3 path for out model artifacts and a role with access to S3 input and output paths.\n",
    "1. Hyperparameters for our neural network.  Since with a 64 dimensional embedding, our recommendations reverted too closely to the mean, let's increase this by an order of magnitude when we train outside of our local instance.  We'll also increase the epochs to see how our accuracy evolves over time. We'll leave all other hyperparameters the same.\n",
    "\n",
    "Once we use `.fit()` this creates a SageMaker Training Job that spins up instances, loads the appropriate packages and data, runs our `train` function from `recommender.py`, wraps up and saves model artifacts to S3, and finishes by tearing down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-823715915892\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-12-10-02-52-40-970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-10 02:52:41 Starting - Starting the training job...\n",
      "2018-12-10 02:52:43 Starting - Launching requested ML instances......\n",
      "2018-12-10 02:53:47 Starting - Preparing the instances for training......\n",
      "2018-12-10 02:55:08 Downloading - Downloading input data...\n",
      "2018-12-10 02:55:37 Training - Downloading the training image.....\n",
      "\u001b[31m2018-12-10 02:56:15,971 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-12-10 02:56:15,971 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-12-10 02:56:15,990 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2018-12-10 02:56:18,610 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'resource_config': {'hosts': ['algo-1'], 'current_host': 'algo-1', 'network_interface_name': 'ethwe'}, 'enable_cloudwatch_metrics': False, '_ps_verbose': 0, 'container_log_level': 20, 'hyperparameters': {'lr': 0.02, 'sagemaker_region': 'us-east-1', 'sagemaker_submit_directory': 's3://sagemaker-us-east-1-823715915892/sagemaker-mxnet-2018-12-10-02-52-40-970/source/sourcedir.tar.gz', 'sagemaker_job_name': 'sagemaker-mxnet-2018-12-10-02-52-40-970', 'sagemaker_program': 'recommender_test_2.py', 'momentum': 0.9, 'num_embeddings': 512, 'sagemaker_container_log_level': 20, 'sagemaker_enable_cloudwatch_metrics': False, 'epochs': 10, 'opt': 'sgd', 'wd': 0.0}, 'current_host': 'algo-1', 'input_dir': '/opt/ml/input', 'base_dir': '/opt/ml', 'user_script_archive': 's3://sagemaker-us-east-1-823715915892/sagemaker-mxnet-2018-12-10-02-52-40-970/source/sourcedir.tar.gz', '_scheduler_host': 'algo-1', 'output_dir': '/opt/ml/output', 'user_script_name': 'recommender_test_2.py', 'hosts': ['algo-1'], 'user_requirements_file': None, '_scheduler_ip': '10.32.0.4', 'sagemaker_region': 'us-east-1', 'job_name': 'sagemaker-mxnet-2018-12-10-02-52-40-970', '_ps_port': 8000, 'available_cpus': 4, 'channels': {'train': {'RecordWrapperType': 'None', 'TrainingInputMode': 'File', 'S3DistributionType': 'FullyReplicated'}}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'available_gpus': 1, 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {'train': '/opt/ml/input/data/train'}, 'model_dir': '/opt/ml/model'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-823715915892/sagemaker-mxnet-2018-12-10-02-52-40-970/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-12-10 02:56:18,880 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31mCollecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/d4/6e9c56a561f1d27407bf29318ca43f36ccaa289271b805a30034eb3a8ec4/pandas-0.23.4-cp35-cp35m-manylinux1_x86_64.whl (8.7MB)\u001b[0m\n",
      "\u001b[31mCollecting pytz>=2011k (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/0e/2365ddc010afb3d79147f1dd544e5ee24bf4ece58ab99b16fbb465ce6dc0/pytz-2018.7-py2.py3-none-any.whl (506kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.13.3)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.3)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\u001b[0m\n",
      "\u001b[31mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\u001b[31mSuccessfully installed pandas-0.23.4 pytz-2018.7\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.0, however version 18.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/mxnet_container/train.py:178: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  train_args = inspect.getargspec(user_module.train)\u001b[0m\n",
      "\u001b[31mepoch: 0\u001b[0m\n",
      "\n",
      "2018-12-10 02:56:16 Training - Training image download completed. Training in progress.\u001b[31mEPOCH 0: MSE ON TRAINING and TEST: 1.1933281402018239. 1.1931668633038235\u001b[0m\n",
      "\u001b[31mepoch: 1\u001b[0m\n",
      "\u001b[31mEPOCH 1: MSE ON TRAINING and TEST: 1.0198700771763936. 1.0197527911955246\u001b[0m\n",
      "\u001b[31mepoch: 2\u001b[0m\n",
      "\u001b[31mEPOCH 2: MSE ON TRAINING and TEST: 0.9587156275396612. 0.9587280049874044\u001b[0m\n",
      "\u001b[31mepoch: 3\u001b[0m\n",
      "\u001b[31mEPOCH 3: MSE ON TRAINING and TEST: 0.9266187946206516. 0.9265599220714903\u001b[0m\n",
      "\u001b[31mepoch: 4\u001b[0m\n",
      "\u001b[31mEPOCH 4: MSE ON TRAINING and TEST: 0.8936502106035038. 0.893547128513839\u001b[0m\n",
      "\u001b[31mepoch: 5\u001b[0m\n",
      "\u001b[31mEPOCH 5: MSE ON TRAINING and TEST: 0.8803157652934475. 0.8803206095980322\u001b[0m\n",
      "\u001b[31mepoch: 6\u001b[0m\n",
      "\u001b[31mEPOCH 6: MSE ON TRAINING and TEST: 0.8463676504936571. 0.8463208064605721\u001b[0m\n",
      "\u001b[31mepoch: 7\u001b[0m\n",
      "\u001b[31mEPOCH 7: MSE ON TRAINING and TEST: 0.8512256370991775. 0.8512226184618842\u001b[0m\n",
      "\u001b[31mepoch: 8\u001b[0m\n",
      "\u001b[31mEPOCH 8: MSE ON TRAINING and TEST: 0.8410577979073097. 0.8409517418466562\u001b[0m\n",
      "\u001b[31mepoch: 9\u001b[0m\n",
      "\u001b[31mEPOCH 9: MSE ON TRAINING and TEST: 0.8262038356734844. 0.8261880280428885\u001b[0m\n",
      "\u001b[31mend of training\u001b[0m\n",
      "\n",
      "2018-12-10 03:16:38 Uploading - Uploading generated training model\n",
      "2018-12-10 03:16:38 Completed - Training job completed\n",
      "Billable seconds: 1290\n"
     ]
    }
   ],
   "source": [
    "m = MXNet('recommender_test_2.py', \n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.p2.xlarge\",\n",
    "          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "          hyperparameters={'num_embeddings': 512, \n",
    "                           'opt': opt, \n",
    "                           'lr': lr, \n",
    "                           'momentum': momentum, \n",
    "                           'wd': wd,\n",
    "                           'epochs': 10},\n",
    "         framework_version='1.1')\n",
    "\n",
    "m.fit({'train': 's3://{}/{}/train/'.format(bucket, prefix)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Now that we've trained our model, deploying it to a real-time, production endpoint is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-mxnet-2018-12-10-02-52-40-970\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-mxnet-2018-12-10-02-52-40-970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, \n",
    "                     instance_type='ml.m4.xlarge')\n",
    "predictor.serializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an endpoint, let's test it out.  We'll predict user #6's ratings for the top and bottom ASINs from our local model.\n",
    "\n",
    "*This could be done by sending HTTP POST requests from a separate web service, but to keep things easy, we'll just use the `.predict()` method from the SageMaker Python SDK.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.3196239471435547, 2.863354206085205]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist(), \n",
    "                              'product_id': [3613, 3548]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(11, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.265327215194702,\n",
       " 2.8325424194335938,\n",
       " 2.378007411956787,\n",
       " 2.158130168914795,\n",
       " 1.8588508367538452,\n",
       " 2.064333200454712,\n",
       " 3.1149203777313232,\n",
       " 2.7032158374786377,\n",
       " 2.510226249694824,\n",
       " 1.9703006744384766,\n",
       " 2.8063013553619385,\n",
       " 3.330688714981079,\n",
       " 1.7892632484436035,\n",
       " 3.20155930519104,\n",
       " 2.5401034355163574,\n",
       " 2.340268850326538,\n",
       " 2.8443186283111572,\n",
       " 3.2755870819091797,\n",
       " 2.729722261428833,\n",
       " 2.3734848499298096,\n",
       " 2.329993724822998,\n",
       " 2.6018197536468506,\n",
       " 2.260087251663208,\n",
       " 2.3417444229125977,\n",
       " 2.572028875350952,\n",
       " 2.6176555156707764,\n",
       " 2.673621892929077,\n",
       " 2.4263052940368652,\n",
       " 3.187283515930176,\n",
       " 3.190189838409424,\n",
       " 2.7514772415161133,\n",
       " 2.858943462371826,\n",
       " 3.339454174041748,\n",
       " 2.936739206314087,\n",
       " 3.371976137161255,\n",
       " 2.9346840381622314,\n",
       " 2.8738186359405518,\n",
       " 2.9037444591522217,\n",
       " 1.8383561372756958,\n",
       " 2.9336671829223633,\n",
       " 3.898113489151001,\n",
       " 3.0374293327331543,\n",
       " 2.6557679176330566,\n",
       " 3.13411021232605,\n",
       " 2.2900960445404053,\n",
       " 2.3741915225982666,\n",
       " 2.053189754486084,\n",
       " 3.545484781265259,\n",
       " 2.6436820030212402,\n",
       " 3.0517382621765137,\n",
       " 3.336989402770996,\n",
       " 3.265327215194702,\n",
       " 2.861435890197754,\n",
       " 3.3449065685272217,\n",
       " 2.3201467990875244,\n",
       " 2.3165640830993652,\n",
       " 2.483686685562134,\n",
       " 2.9730820655822754,\n",
       " 2.9278624057769775,\n",
       " 2.644059658050537,\n",
       " 2.4014692306518555,\n",
       " 2.60038161277771,\n",
       " 2.7472898960113525,\n",
       " 2.445380449295044,\n",
       " 1.7290465831756592,\n",
       " 1.4520657062530518,\n",
       " 1.5203452110290527,\n",
       " 3.2119314670562744,\n",
       " 2.4030847549438477,\n",
       " 3.1962573528289795,\n",
       " 2.57151198387146,\n",
       " 1.5740129947662354,\n",
       " 2.947953462600708,\n",
       " 2.5652146339416504,\n",
       " 2.5511741638183594,\n",
       " 2.545074701309204,\n",
       " 2.2220752239227295,\n",
       " 3.2472593784332275,\n",
       " 2.5449728965759277,\n",
       " 2.032843828201294,\n",
       " 3.60170316696167,\n",
       " 2.821139097213745,\n",
       " 3.0294480323791504,\n",
       " 3.544633150100708,\n",
       " 2.2163188457489014,\n",
       " 2.098123550415039,\n",
       " 2.5703518390655518,\n",
       " 1.8684730529785156,\n",
       " 2.091287136077881,\n",
       " 2.183817148208618,\n",
       " 3.4930338859558105,\n",
       " 3.265327215194702,\n",
       " 2.242405891418457,\n",
       " 1.6580557823181152,\n",
       " 3.221681833267212,\n",
       " 1.9432076215744019,\n",
       " 3.7492246627807617,\n",
       " 3.582005262374878,\n",
       " 2.2346065044403076,\n",
       " 3.03090500831604,\n",
       " 2.260854482650757,\n",
       " 3.7930660247802734,\n",
       " 1.5140403509140015,\n",
       " 2.569894313812256,\n",
       " 2.451978921890259,\n",
       " 3.0275514125823975,\n",
       " 3.7369468212127686,\n",
       " 2.5193111896514893,\n",
       " 2.7416305541992188,\n",
       " 3.265327215194702,\n",
       " 2.91080641746521,\n",
       " 3.5173587799072266,\n",
       " 3.268123149871826,\n",
       " 2.8658456802368164,\n",
       " 2.5661377906799316,\n",
       " 3.265327215194702,\n",
       " 3.341512441635132,\n",
       " 2.696241617202759,\n",
       " 2.0882482528686523,\n",
       " 2.935917854309082,\n",
       " 3.6928813457489014,\n",
       " 2.800138235092163,\n",
       " 1.8278446197509766,\n",
       " 3.3888611793518066,\n",
       " 3.2056045532226562,\n",
       " 3.199932813644409,\n",
       " 1.8300575017929077,\n",
       " 3.799699306488037,\n",
       " 3.4287943840026855,\n",
       " 2.5197854042053223,\n",
       " 2.946563482284546,\n",
       " 3.606194019317627,\n",
       " 1.5976752042770386,\n",
       " 3.642258882522583,\n",
       " 4.2242231369018555,\n",
       " 2.2132859230041504,\n",
       " 2.90814208984375,\n",
       " 3.2480082511901855,\n",
       " 2.335996150970459,\n",
       " 2.5911149978637695,\n",
       " 2.188971519470215,\n",
       " 2.4311599731445312,\n",
       " 3.6434645652770996,\n",
       " 3.265327215194702,\n",
       " 2.759791612625122,\n",
       " 2.7605154514312744,\n",
       " 2.983211040496826,\n",
       " 2.220165729522705,\n",
       " 2.4185919761657715,\n",
       " 3.5281829833984375,\n",
       " 3.1415324211120605,\n",
       " 2.3376476764678955,\n",
       " 2.454735517501831,\n",
       " 2.17075514793396,\n",
       " 2.693098783493042,\n",
       " 2.8741111755371094,\n",
       " 2.9375035762786865,\n",
       " 2.252852439880371,\n",
       " 2.67537784576416,\n",
       " 2.72660493850708,\n",
       " 1.912446141242981,\n",
       " 2.79423451423645,\n",
       " 2.5121264457702637,\n",
       " 2.719545602798462,\n",
       " 3.1782312393188477,\n",
       " 2.767059087753296,\n",
       " 2.259230613708496,\n",
       " 3.411545991897583,\n",
       " 2.0989296436309814,\n",
       " 1.4530850648880005,\n",
       " 2.390080690383911,\n",
       " 2.9895246028900146,\n",
       " 2.102271318435669,\n",
       " 1.8549253940582275,\n",
       " 1.7193589210510254,\n",
       " 2.803738832473755,\n",
       " 3.3524436950683594,\n",
       " 2.6057887077331543,\n",
       " 2.5384414196014404,\n",
       " 1.3615907430648804,\n",
       " 3.0120694637298584,\n",
       " 0.9983900189399719,\n",
       " 2.8473525047302246,\n",
       " 2.7781944274902344,\n",
       " 2.530820369720459,\n",
       " 2.484076976776123,\n",
       " 2.1288561820983887,\n",
       " 2.4579873085021973,\n",
       " 2.473606824874878,\n",
       " 3.107851505279541,\n",
       " 3.491495370864868,\n",
       " 2.042621612548828,\n",
       " 3.097926378250122,\n",
       " 1.5019749402999878,\n",
       " 3.1335690021514893,\n",
       " 2.4861831665039062,\n",
       " 2.0483736991882324,\n",
       " 2.8078081607818604,\n",
       " 2.9837474822998047,\n",
       " 3.34778094291687,\n",
       " 2.740067720413208,\n",
       " 1.9567203521728516,\n",
       " 1.9177453517913818,\n",
       " 2.7220542430877686,\n",
       " 2.110614061355591,\n",
       " 2.7508115768432617,\n",
       " 2.4274439811706543,\n",
       " 2.400845766067505,\n",
       " 2.1480050086975098,\n",
       " 2.138627052307129,\n",
       " 2.551396131515503,\n",
       " 2.958495616912842,\n",
       " 2.0984926223754883,\n",
       " 3.331615686416626,\n",
       " 2.9710540771484375,\n",
       " 2.9601433277130127,\n",
       " 2.1624321937561035,\n",
       " 2.159982681274414,\n",
       " 2.8369898796081543,\n",
       " 2.671550989151001,\n",
       " 1.944046974182129,\n",
       " 3.265327215194702,\n",
       " 2.978498935699463,\n",
       " 3.3421542644500732,\n",
       " 2.9472711086273193,\n",
       " 2.389970064163208,\n",
       " 2.2606353759765625,\n",
       " 2.209202289581299,\n",
       " 2.0942325592041016,\n",
       " 2.549497127532959,\n",
       " 3.1563880443573,\n",
       " 2.299595594406128,\n",
       " 3.029067277908325,\n",
       " 2.6190128326416016,\n",
       " 1.78643798828125,\n",
       " 2.944873094558716,\n",
       " 2.564732074737549,\n",
       " 2.5679516792297363,\n",
       " 2.477393388748169,\n",
       " 2.354091167449951,\n",
       " 2.1933608055114746,\n",
       " 2.8804659843444824,\n",
       " 2.694556474685669,\n",
       " 2.315582275390625,\n",
       " 3.1862986087799072,\n",
       " 3.4795169830322266,\n",
       " 3.363778829574585,\n",
       " 2.4045023918151855,\n",
       " 2.483952522277832,\n",
       " 3.1005444526672363,\n",
       " 2.324308395385742,\n",
       " 3.306210994720459,\n",
       " 2.479367971420288,\n",
       " 2.8776118755340576,\n",
       " 2.912339687347412,\n",
       " 1.4464741945266724,\n",
       " 2.0173237323760986,\n",
       " 2.6527180671691895,\n",
       " 1.8932607173919678,\n",
       " 2.599138021469116,\n",
       " 3.580883026123047,\n",
       " 2.6268763542175293,\n",
       " 2.8994946479797363,\n",
       " 3.271868944168091,\n",
       " 3.93261981010437,\n",
       " 3.0845649242401123,\n",
       " 2.873436450958252,\n",
       " 2.5279173851013184,\n",
       " 2.1770269870758057,\n",
       " 2.4597694873809814,\n",
       " 2.071727991104126,\n",
       " 2.221112012863159,\n",
       " 2.58232045173645,\n",
       " 2.5063533782958984,\n",
       " 2.235004186630249,\n",
       " 1.7937865257263184,\n",
       " 1.9669476747512817,\n",
       " 2.8152859210968018,\n",
       " 2.0349411964416504,\n",
       " 2.94044828414917,\n",
       " 3.3055760860443115,\n",
       " 2.5290441513061523,\n",
       " 2.666323184967041,\n",
       " 2.902904987335205,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 4.426406383514404,\n",
       " 2.876538038253784,\n",
       " 2.091881513595581,\n",
       " 2.66512393951416,\n",
       " 3.228060722351074,\n",
       " 2.1392898559570312,\n",
       " 2.410921335220337,\n",
       " 3.1127257347106934,\n",
       " 2.8379733562469482,\n",
       " 2.120415449142456,\n",
       " 3.1473920345306396,\n",
       " 2.2691924571990967,\n",
       " 2.6606075763702393,\n",
       " 3.009162187576294,\n",
       " 2.8500452041625977,\n",
       " 2.069673776626587,\n",
       " 3.443868398666382,\n",
       " 2.2728395462036133,\n",
       " 2.619866132736206,\n",
       " 1.9948852062225342,\n",
       " 3.489459991455078,\n",
       " 3.5388262271881104,\n",
       " 3.2574243545532227,\n",
       " 2.6590754985809326,\n",
       " 2.085343360900879,\n",
       " 2.7503104209899902,\n",
       " 2.207977533340454,\n",
       " 1.9827595949172974,\n",
       " 3.1621451377868652,\n",
       " 2.2857627868652344,\n",
       " 2.811955690383911,\n",
       " 2.3296189308166504,\n",
       " 4.017237663269043,\n",
       " 2.6490724086761475,\n",
       " 2.915849208831787,\n",
       " 2.3869237899780273,\n",
       " 2.689964771270752,\n",
       " 3.265327215194702,\n",
       " 2.9887592792510986,\n",
       " 1.5266255140304565,\n",
       " 2.7120208740234375,\n",
       " 1.9846086502075195,\n",
       " 1.9314521551132202,\n",
       " 2.6787667274475098,\n",
       " 1.702686071395874,\n",
       " 2.54599666595459,\n",
       " 1.8835296630859375,\n",
       " 3.273319721221924,\n",
       " 3.068697214126587,\n",
       " 3.193096160888672,\n",
       " 2.3362860679626465,\n",
       " 3.1303892135620117,\n",
       " 2.681811809539795,\n",
       " 2.7306137084960938,\n",
       " 2.2339630126953125,\n",
       " 3.0378899574279785,\n",
       " 3.1667699813842773,\n",
       " 2.048349380493164,\n",
       " 2.2124674320220947,\n",
       " 3.391937017440796,\n",
       " 2.6107380390167236,\n",
       " 2.852741003036499,\n",
       " 3.14595103263855,\n",
       " 3.0583343505859375,\n",
       " 2.680270195007324,\n",
       " 2.4666500091552734,\n",
       " 2.619741439819336,\n",
       " 2.7464826107025146,\n",
       " 2.8125603199005127,\n",
       " 2.034224510192871,\n",
       " 3.0118460655212402,\n",
       " 3.436408519744873,\n",
       " 2.2778687477111816,\n",
       " 2.515225887298584,\n",
       " 2.0836522579193115,\n",
       " 2.686246633529663,\n",
       " 2.5582432746887207,\n",
       " 3.324385166168213,\n",
       " 3.2520275115966797,\n",
       " 2.507023334503174,\n",
       " 2.191253662109375,\n",
       " 2.5287277698516846,\n",
       " 2.8123507499694824,\n",
       " 2.4652371406555176,\n",
       " 2.3924169540405273,\n",
       " 2.664858818054199,\n",
       " 2.7396459579467773,\n",
       " 3.3229005336761475,\n",
       " 1.4361615180969238,\n",
       " 3.0850329399108887,\n",
       " 2.9468889236450195,\n",
       " 2.7263247966766357,\n",
       " 2.5557007789611816,\n",
       " 2.2051637172698975,\n",
       " 2.9872219562530518,\n",
       " 2.6505651473999023,\n",
       " 2.208258628845215,\n",
       " 2.561906337738037,\n",
       " 2.531667947769165,\n",
       " 2.8178725242614746,\n",
       " 2.135075092315674,\n",
       " 2.1813576221466064,\n",
       " 3.2981135845184326,\n",
       " 3.535224199295044,\n",
       " 2.598964214324951,\n",
       " 2.3626303672790527,\n",
       " 2.190453290939331,\n",
       " 1.5133932828903198,\n",
       " 3.5546367168426514,\n",
       " 3.265327215194702,\n",
       " 2.3410041332244873,\n",
       " 2.1797499656677246,\n",
       " 2.7992265224456787,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 3.468409538269043,\n",
       " 2.997044801712036,\n",
       " 3.265327215194702,\n",
       " 3.9568839073181152,\n",
       " 1.6795454025268555,\n",
       " 2.6588635444641113,\n",
       " 2.277906894683838,\n",
       " 2.182332992553711,\n",
       " 2.497706413269043,\n",
       " 2.3320510387420654,\n",
       " 3.0073444843292236,\n",
       " 2.5545742511749268,\n",
       " 2.036527633666992,\n",
       " 2.3569774627685547,\n",
       " 2.064401388168335,\n",
       " 2.543123483657837,\n",
       " 2.7335026264190674,\n",
       " 1.6026543378829956,\n",
       " 1.6802278757095337,\n",
       " 2.03704833984375,\n",
       " 2.7209417819976807,\n",
       " 2.8598387241363525,\n",
       " 2.307048797607422,\n",
       " 2.2514843940734863,\n",
       " 2.8734891414642334,\n",
       " 2.581205368041992,\n",
       " 1.4097726345062256,\n",
       " 3.607790470123291,\n",
       " 1.7215608358383179,\n",
       " 2.0545623302459717,\n",
       " 3.1388845443725586,\n",
       " 1.6021332740783691,\n",
       " 2.547968864440918,\n",
       " 2.9386868476867676,\n",
       " 2.4571268558502197,\n",
       " 2.270801544189453,\n",
       " 1.1449475288391113,\n",
       " 2.2462551593780518,\n",
       " 2.7044453620910645,\n",
       " 3.0835118293762207,\n",
       " 2.868029832839966,\n",
       " 2.082395076751709,\n",
       " 2.376882314682007,\n",
       " 1.654037594795227,\n",
       " 2.303358554840088,\n",
       " 3.2241036891937256,\n",
       " 1.5558021068572998,\n",
       " 2.604823589324951,\n",
       " 3.1137585639953613,\n",
       " 2.5097970962524414,\n",
       " 2.5959417819976807,\n",
       " 2.113722562789917,\n",
       " 2.3084442615509033,\n",
       " 3.3658077716827393,\n",
       " 1.8920676708221436,\n",
       " 3.36261248588562,\n",
       " 3.066803455352783,\n",
       " 2.5093328952789307,\n",
       " 1.5828428268432617,\n",
       " 1.8400251865386963,\n",
       " 2.7668042182922363,\n",
       " 2.880056619644165,\n",
       " 2.5341734886169434,\n",
       " 2.204530715942383,\n",
       " 2.677535057067871,\n",
       " 1.990110993385315,\n",
       " 2.502126693725586,\n",
       " 2.6817078590393066,\n",
       " 2.9163503646850586,\n",
       " 1.4663153886795044,\n",
       " 3.4768433570861816,\n",
       " 2.4094200134277344,\n",
       " 1.9916330575942993,\n",
       " 3.332989454269409,\n",
       " 3.2938365936279297,\n",
       " 1.9352803230285645,\n",
       " 2.159207820892334,\n",
       " 1.9585084915161133,\n",
       " 2.2670063972473145,\n",
       " 3.204821825027466,\n",
       " 2.5814926624298096,\n",
       " 2.760923147201538,\n",
       " 3.0873422622680664,\n",
       " 2.2079896926879883,\n",
       " 1.9443598985671997,\n",
       " 2.2112045288085938,\n",
       " 2.224749803543091,\n",
       " 2.5439915657043457,\n",
       " 2.012753486633301,\n",
       " 2.727386951446533,\n",
       " 2.595946788787842,\n",
       " 2.902691602706909,\n",
       " 2.5546348094940186,\n",
       " 2.4288415908813477,\n",
       " 2.9429879188537598,\n",
       " 2.558664083480835,\n",
       " 3.0196077823638916,\n",
       " 2.521754741668701,\n",
       " 2.810371160507202,\n",
       " 2.629124402999878,\n",
       " 2.888410806655884,\n",
       " 1.590922474861145,\n",
       " 3.5398457050323486,\n",
       " 2.4616267681121826,\n",
       " 2.0236868858337402,\n",
       " 3.055723190307617,\n",
       " 3.0486841201782227,\n",
       " 3.6665761470794678,\n",
       " 2.875035285949707,\n",
       " 2.4908313751220703,\n",
       " 2.1914165019989014,\n",
       " 2.5032386779785156,\n",
       " 2.630161762237549,\n",
       " 2.5094127655029297,\n",
       " 3.771775007247925,\n",
       " 2.1718640327453613,\n",
       " 2.9765193462371826,\n",
       " 2.0959646701812744,\n",
       " 1.421053409576416,\n",
       " 2.2833211421966553,\n",
       " 2.600281238555908,\n",
       " 2.8693268299102783,\n",
       " 3.2129783630371094,\n",
       " 3.040863037109375,\n",
       " 2.3261163234710693,\n",
       " 3.0179450511932373,\n",
       " 4.038342475891113,\n",
       " 2.2028002738952637,\n",
       " 3.297393560409546,\n",
       " 3.3521482944488525,\n",
       " 2.930121660232544,\n",
       " 2.1666088104248047,\n",
       " 2.59028697013855,\n",
       " 3.2682886123657227,\n",
       " 2.894473075866699,\n",
       " 2.8781871795654297,\n",
       " 2.5243425369262695,\n",
       " 2.8627123832702637,\n",
       " 2.9738292694091797,\n",
       " 1.3777985572814941,\n",
       " 3.6232950687408447,\n",
       " 2.3780767917633057,\n",
       " 2.366529941558838,\n",
       " 2.1234066486358643,\n",
       " 3.0240838527679443,\n",
       " 1.1230727434158325,\n",
       " 2.0064785480499268,\n",
       " 1.9888792037963867,\n",
       " 2.837118148803711,\n",
       " 2.7098827362060547,\n",
       " 3.436721086502075,\n",
       " 2.16892409324646,\n",
       " 3.0476152896881104,\n",
       " 2.185527801513672,\n",
       " 3.2224857807159424,\n",
       " 3.59451961517334,\n",
       " 4.067449569702148,\n",
       " 2.2208290100097656,\n",
       " 2.0582644939422607,\n",
       " 2.117128372192383,\n",
       " 2.974977970123291,\n",
       " 3.1409428119659424,\n",
       " 2.352890729904175,\n",
       " 2.019392251968384,\n",
       " 2.5157277584075928,\n",
       " 2.3632235527038574,\n",
       " 2.6444077491760254,\n",
       " 2.529893398284912,\n",
       " 2.1266908645629883,\n",
       " 2.990324020385742,\n",
       " 2.5464508533477783,\n",
       " 3.267584800720215,\n",
       " 2.8151609897613525,\n",
       " 2.7635409832000732,\n",
       " 2.029911994934082,\n",
       " 4.054963111877441,\n",
       " 1.7832939624786377,\n",
       " 3.9151804447174072,\n",
       " 2.7943673133850098,\n",
       " 2.7070536613464355,\n",
       " 3.3571672439575195,\n",
       " 2.823986768722534,\n",
       " 3.750457763671875,\n",
       " 4.244679927825928,\n",
       " 2.039496421813965,\n",
       " 2.4847583770751953,\n",
       " 2.8043904304504395,\n",
       " 3.374652147293091,\n",
       " 3.049325466156006,\n",
       " 3.103886365890503,\n",
       " 4.281132698059082,\n",
       " 2.799546003341675,\n",
       " 3.4771621227264404,\n",
       " 3.023538589477539,\n",
       " 3.155433416366577,\n",
       " 2.9208383560180664,\n",
       " 2.456470012664795,\n",
       " 2.9803407192230225,\n",
       " 3.1856532096862793,\n",
       " 2.920077323913574,\n",
       " 2.4987692832946777,\n",
       " 3.791350841522217,\n",
       " 2.3255741596221924,\n",
       " 3.265327215194702,\n",
       " 2.6722428798675537,\n",
       " 1.776073694229126,\n",
       " 2.3630917072296143,\n",
       " 2.9878652095794678,\n",
       " 1.8899849653244019,\n",
       " 2.4835236072540283,\n",
       " 2.181654930114746,\n",
       " 1.821691632270813,\n",
       " 3.122227430343628,\n",
       " 2.637678384780884,\n",
       " 3.1558895111083984,\n",
       " 2.939476251602173,\n",
       " 2.5922603607177734,\n",
       " 3.521862268447876,\n",
       " 2.3193066120147705,\n",
       " 3.265327215194702,\n",
       " 3.2286038398742676,\n",
       " 3.265327215194702,\n",
       " 3.54911732673645,\n",
       " 4.440979957580566,\n",
       " 3.265327215194702,\n",
       " 1.9974724054336548,\n",
       " 3.4071924686431885,\n",
       " 3.3117849826812744,\n",
       " 3.265327215194702,\n",
       " 2.588008403778076,\n",
       " 1.671681523323059,\n",
       " 3.247929573059082,\n",
       " 2.6454317569732666,\n",
       " 2.248018741607666,\n",
       " 2.6588993072509766,\n",
       " 3.265327215194702,\n",
       " 2.072922945022583,\n",
       " 2.176323413848877,\n",
       " 2.2825846672058105,\n",
       " 1.6245815753936768,\n",
       " 2.2239606380462646,\n",
       " 4.168849468231201,\n",
       " 2.6568431854248047,\n",
       " 3.1963303089141846,\n",
       " 2.8371012210845947,\n",
       " 3.265327215194702,\n",
       " 2.8444108963012695,\n",
       " 2.7152597904205322,\n",
       " 4.173935413360596,\n",
       " 2.2249739170074463,\n",
       " 3.0573623180389404,\n",
       " 2.732053279876709,\n",
       " 2.498924493789673,\n",
       " 3.265327215194702,\n",
       " 3.9573841094970703,\n",
       " 2.9077959060668945,\n",
       " 2.9358112812042236,\n",
       " 4.217353343963623,\n",
       " 3.054884672164917,\n",
       " 2.6287033557891846,\n",
       " 2.6436960697174072,\n",
       " 2.373782157897949,\n",
       " 2.8773868083953857,\n",
       " 2.2762527465820312,\n",
       " 2.806375741958618,\n",
       " 3.0415546894073486,\n",
       " 2.2121503353118896,\n",
       " 3.503748893737793,\n",
       " 3.6964008808135986,\n",
       " 3.560828924179077,\n",
       " 3.1287178993225098,\n",
       " 2.53316593170166,\n",
       " 2.0380806922912598,\n",
       " 2.1844735145568848,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 3.2362372875213623,\n",
       " 3.4250192642211914,\n",
       " 3.0669469833374023,\n",
       " 2.7904000282287598,\n",
       " 2.1364855766296387,\n",
       " 3.265327215194702,\n",
       " 3.5577046871185303,\n",
       " 2.7585713863372803,\n",
       " 3.265327215194702,\n",
       " 2.4506423473358154,\n",
       " 1.6990315914154053,\n",
       " 3.265327215194702,\n",
       " 2.7238075733184814,\n",
       " 2.4061405658721924,\n",
       " 1.6977308988571167,\n",
       " 3.265327215194702,\n",
       " 1.9654861688613892,\n",
       " 2.3782622814178467,\n",
       " 4.083786964416504,\n",
       " 2.6130008697509766,\n",
       " 2.5801126956939697,\n",
       " 3.265327215194702,\n",
       " 2.47619891166687,\n",
       " 3.470099687576294,\n",
       " 2.5430078506469727,\n",
       " 1.916089415550232,\n",
       " 1.8362014293670654,\n",
       " 2.7384305000305176,\n",
       " 3.7866060733795166,\n",
       " 2.3721022605895996,\n",
       " 3.4545066356658936,\n",
       " 3.151479959487915,\n",
       " 1.54495108127594,\n",
       " 2.256885051727295,\n",
       " 2.4577808380126953,\n",
       " 3.265327215194702,\n",
       " 2.97104549407959,\n",
       " 2.0273451805114746,\n",
       " 2.482001543045044,\n",
       " 3.701235771179199,\n",
       " 3.040850877761841,\n",
       " 2.1276512145996094,\n",
       " 3.7338125705718994,\n",
       " 3.265327215194702,\n",
       " 2.4363155364990234,\n",
       " 3.265327215194702,\n",
       " 2.2548913955688477,\n",
       " 2.036586284637451,\n",
       " 3.043196201324463,\n",
       " 3.265327215194702,\n",
       " 2.4857640266418457,\n",
       " 3.813952684402466,\n",
       " 2.125321865081787,\n",
       " 2.2397637367248535,\n",
       " 3.670917510986328,\n",
       " 3.0020811557769775,\n",
       " 2.3145666122436523,\n",
       " 2.4983725547790527,\n",
       " 2.6789462566375732,\n",
       " 1.755601406097412,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 2.8227362632751465,\n",
       " 2.0530290603637695,\n",
       " 2.2620203495025635,\n",
       " 2.801685094833374,\n",
       " 3.7826194763183594,\n",
       " 3.0072414875030518,\n",
       " 1.6879252195358276,\n",
       " 2.5471770763397217,\n",
       " 2.652935028076172,\n",
       " 3.616666078567505,\n",
       " 3.0910894870758057,\n",
       " 3.265327215194702,\n",
       " 2.4385430812835693,\n",
       " 2.994197130203247,\n",
       " 2.6500470638275146,\n",
       " 2.4925167560577393,\n",
       " 3.3810274600982666,\n",
       " 3.532510280609131,\n",
       " 2.953888416290283,\n",
       " 2.8935632705688477,\n",
       " 2.5846946239471436,\n",
       " 1.4860495328903198,\n",
       " 2.650831699371338,\n",
       " 2.8116376399993896,\n",
       " 2.548213481903076,\n",
       " 2.9712698459625244,\n",
       " 2.6991140842437744,\n",
       " 3.265327215194702,\n",
       " 3.3011701107025146,\n",
       " 3.265327215194702,\n",
       " 3.5244650840759277,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 2.5919198989868164,\n",
       " 3.9355411529541016,\n",
       " 2.9154834747314453,\n",
       " 3.265327215194702,\n",
       " 2.7517569065093994,\n",
       " 1.9626888036727905,\n",
       " 2.7619311809539795,\n",
       " 2.2974905967712402,\n",
       " 1.9894362688064575,\n",
       " 2.776775598526001,\n",
       " 2.0928139686584473,\n",
       " 2.935230255126953,\n",
       " 2.22357177734375,\n",
       " 3.406724452972412,\n",
       " 2.3377790451049805,\n",
       " 3.623842716217041,\n",
       " 3.4898343086242676,\n",
       " 2.48240327835083,\n",
       " 3.8980815410614014,\n",
       " 2.813657283782959,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 2.5466043949127197,\n",
       " 3.265327215194702,\n",
       " 2.3091042041778564,\n",
       " 2.3469314575195312,\n",
       " 3.4376089572906494,\n",
       " 2.711530923843384,\n",
       " 2.7978146076202393,\n",
       " 2.5262999534606934,\n",
       " 1.9926097393035889,\n",
       " 2.6557343006134033,\n",
       " 2.5993611812591553,\n",
       " 2.392332077026367,\n",
       " 2.545275926589966,\n",
       " 1.9706356525421143,\n",
       " 1.030644178390503,\n",
       " 3.6377828121185303,\n",
       " 3.265327215194702,\n",
       " 2.561495304107666,\n",
       " 2.964837074279785,\n",
       " 2.5987112522125244,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 2.0323097705841064,\n",
       " 3.265327215194702,\n",
       " 3.3604323863983154,\n",
       " 2.953801155090332,\n",
       " 3.265327215194702,\n",
       " 2.946580410003662,\n",
       " 3.0305233001708984,\n",
       " 3.265327215194702,\n",
       " 2.8357913494110107,\n",
       " 3.2471933364868164,\n",
       " 2.4472568035125732,\n",
       " 1.7211692333221436,\n",
       " 2.7200779914855957,\n",
       " 2.8948299884796143,\n",
       " 3.020289421081543,\n",
       " 1.8299285173416138,\n",
       " 2.5312323570251465,\n",
       " 2.0172624588012695,\n",
       " 2.3065648078918457,\n",
       " 2.617353916168213,\n",
       " 3.5881216526031494,\n",
       " 1.6837882995605469,\n",
       " 2.6870298385620117,\n",
       " 3.631561517715454,\n",
       " 2.0379796028137207,\n",
       " 3.2281241416931152,\n",
       " 3.8723318576812744,\n",
       " 3.265327215194702,\n",
       " 3.1478612422943115,\n",
       " 3.0422708988189697,\n",
       " 2.535534143447876,\n",
       " 1.781994342803955,\n",
       " 3.300522565841675,\n",
       " 3.0951731204986572,\n",
       " 2.7258172035217285,\n",
       " 2.9813766479492188,\n",
       " 2.6890296936035156,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 3.265327215194702,\n",
       " 3.8441901206970215,\n",
       " 3.3025403022766113,\n",
       " 2.970695972442627,\n",
       " 2.7490129470825195,\n",
       " 2.6632027626037598,\n",
       " 2.74180006980896,\n",
       " 3.305655002593994,\n",
       " 2.4574503898620605,\n",
       " 2.959120035171509,\n",
       " 1.845400094985962,\n",
       " 4.03917932510376,\n",
       " 2.339665174484253,\n",
       " 2.0178141593933105,\n",
       " 3.265327215194702,\n",
       " 3.468386173248291,\n",
       " 3.265327215194702,\n",
       " 3.31097412109375,\n",
       " 2.6320106983184814,\n",
       " 2.6628475189208984,\n",
       " 2.7188796997070312,\n",
       " 3.584982395172119,\n",
       " 2.060331344604492,\n",
       " 1.7872090339660645,\n",
       " 1.5760952234268188,\n",
       " 2.487881660461426,\n",
       " 3.265327215194702,\n",
       " 2.6839439868927,\n",
       " 1.97568941116333,\n",
       " 1.8028877973556519,\n",
       " 3.828495502471924,\n",
       " 2.3793869018554688,\n",
       " 3.0478529930114746,\n",
       " 3.265327215194702,\n",
       " 1.874916434288025,\n",
       " 2.6479482650756836,\n",
       " 3.1355419158935547,\n",
       " 3.265327215194702,\n",
       " 2.5904195308685303,\n",
       " 3.0717387199401855,\n",
       " 3.156507968902588,\n",
       " 3.708793878555298,\n",
       " 3.6951911449432373,\n",
       " 3.183137893676758,\n",
       " 2.941358804702759,\n",
       " 2.7242956161499023,\n",
       " 2.980189323425293,\n",
       " 3.6749303340911865,\n",
       " 3.4307849407196045,\n",
       " 3.382751941680908,\n",
       " 3.0577690601348877,\n",
       " 3.871732234954834,\n",
       " 3.2567174434661865,\n",
       " 2.92777943611145,\n",
       " 2.9273922443389893,\n",
       " 3.7510852813720703,\n",
       " 3.435082197189331,\n",
       " 3.3586673736572266,\n",
       " 3.2949724197387695,\n",
       " 3.951511859893799,\n",
       " 2.9361472129821777,\n",
       " 2.920027017593384,\n",
       " 3.227623462677002,\n",
       " 3.069878578186035,\n",
       " 3.266221284866333,\n",
       " 3.4873039722442627,\n",
       " 3.6753482818603516,\n",
       " 3.359987497329712,\n",
       " 2.6028940677642822,\n",
       " 3.746790885925293,\n",
       " 2.7474617958068848,\n",
       " 3.2024011611938477,\n",
       " 3.487478256225586,\n",
       " 3.864760160446167,\n",
       " 3.2932119369506836,\n",
       " 3.207850217819214,\n",
       " 3.5581750869750977,\n",
       " 2.574676513671875,\n",
       " 2.5402448177337646,\n",
       " 3.71130633354187,\n",
       " 3.3838913440704346,\n",
       " 3.3083112239837646,\n",
       " 2.711322784423828,\n",
       " 3.01878023147583,\n",
       " 3.048490285873413,\n",
       " 3.232618570327759,\n",
       " 3.1030499935150146,\n",
       " 3.179541826248169,\n",
       " 3.703535318374634,\n",
       " 3.2147347927093506,\n",
       " 3.7448883056640625,\n",
       " 2.939516544342041,\n",
       " 3.1901519298553467,\n",
       " 3.051394462585449,\n",
       " 3.66507887840271,\n",
       " 2.3119683265686035,\n",
       " 3.1042025089263916,\n",
       " 3.1173629760742188,\n",
       " 3.1728103160858154,\n",
       " 2.295881509780884,\n",
       " 2.9021167755126953,\n",
       " 3.2051467895507812,\n",
       " 3.1340372562408447,\n",
       " 3.4689550399780273,\n",
       " 2.4749550819396973,\n",
       " 3.1191718578338623,\n",
       " 2.9945015907287598,\n",
       " 2.544511556625366,\n",
       " 3.6225719451904297,\n",
       " 3.5518949031829834,\n",
       " 3.4628615379333496,\n",
       " 2.6519339084625244,\n",
       " 3.7143948078155518,\n",
       " 2.8083183765411377,\n",
       " 3.514782190322876,\n",
       " 2.6052942276000977,\n",
       " 2.885295867919922,\n",
       " 2.7292957305908203,\n",
       " 2.6194956302642822,\n",
       " 3.094590902328491,\n",
       " 3.5986313819885254,\n",
       " 3.217923164367676,\n",
       " 3.265327215194702,\n",
       " 3.6901111602783203,\n",
       " 2.8410680294036865,\n",
       " 3.196035385131836,\n",
       " 3.265327215194702,\n",
       " 2.1938188076019287,\n",
       " 2.429100751876831,\n",
       " 3.066185712814331,\n",
       " 2.7449607849121094,\n",
       " 2.5580296516418457,\n",
       " 2.752516984939575,\n",
       " 1.6729254722595215,\n",
       " 2.594419240951538,\n",
       " 2.883665084838867,\n",
       " 2.69227933883667,\n",
       " 3.5444650650024414,\n",
       " 3.265327215194702,\n",
       " 2.1019227504730225,\n",
       " 2.0703394412994385,\n",
       " 2.1903181076049805,\n",
       " 2.850874900817871,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist(),\n",
    "                             'product_id':list(range(0,3000))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note, some of our predictions are actually greater than 5, which is to be expected as we didn't do anything special to account for ratings being capped at that value.  Since we are only looking to ranking by predicted rating, this won't create problems for our specific use case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Let's start by calculating a naive baseline to approximate how well our model is doing.  The simplest estimate would be to assume every user item rating is just the average rating over all ratings.\n",
    "\n",
    "*Note, we could do better by using each individual video's average, however, in this case it doesn't really matter as the same conclusions would hold.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive MSE: 1.6737109919841058\n"
     ]
    }
   ],
   "source": [
    "print('Naive MSE:', np.mean((test_df['star_rating'] - np.mean(train_df['star_rating'])) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll calculate predictions for our test dataset.\n",
    "\n",
    "*Note, this will align closely to our CloudWatch output above, but may differ slightly due to skipping partial mini-batches in our eval_net function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.3479642876799973\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for array in np.array_split(test_df[['customer_id', 'product_id']].values, 40):\n",
    "    test_preds += predictor.predict(json.dumps({'customer_id': array[:, 0].tolist(), \n",
    "                                                'product_id': array[:, 1].tolist()}))\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "print('MSE:', np.mean((test_df['star_rating'] - test_preds) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our neural network and embedding model produces substantially better results (~1.27 vs 1.65 on mean square error).\n",
    "\n",
    "For recommender systems, subjective accuracy also matters.  Let's get some recommendations for a random user to see if they make intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136535</th>\n",
       "      <td>3618</td>\n",
       "      <td>593</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190544</th>\n",
       "      <td>3618</td>\n",
       "      <td>858</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219754</th>\n",
       "      <td>3618</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600199</th>\n",
       "      <td>3618</td>\n",
       "      <td>1304</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340827</th>\n",
       "      <td>3618</td>\n",
       "      <td>1358</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505978</th>\n",
       "      <td>3618</td>\n",
       "      <td>2324</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453750</th>\n",
       "      <td>3618</td>\n",
       "      <td>1234</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16115</th>\n",
       "      <td>3618</td>\n",
       "      <td>1035</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612908</th>\n",
       "      <td>3618</td>\n",
       "      <td>1947</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591457</th>\n",
       "      <td>3618</td>\n",
       "      <td>899</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563120</th>\n",
       "      <td>3618</td>\n",
       "      <td>953</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695661</th>\n",
       "      <td>3618</td>\n",
       "      <td>1952</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>3618</td>\n",
       "      <td>914</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158429</th>\n",
       "      <td>3618</td>\n",
       "      <td>2501</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617517</th>\n",
       "      <td>3618</td>\n",
       "      <td>508</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152122</th>\n",
       "      <td>3618</td>\n",
       "      <td>1124</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162054</th>\n",
       "      <td>3618</td>\n",
       "      <td>1096</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902041</th>\n",
       "      <td>3618</td>\n",
       "      <td>3007</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693795</th>\n",
       "      <td>3618</td>\n",
       "      <td>2875</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831150</th>\n",
       "      <td>3618</td>\n",
       "      <td>1286</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852019</th>\n",
       "      <td>3618</td>\n",
       "      <td>3194</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>3618</td>\n",
       "      <td>938</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999931</th>\n",
       "      <td>3618</td>\n",
       "      <td>572</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107701</th>\n",
       "      <td>3618</td>\n",
       "      <td>2858</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47411</th>\n",
       "      <td>3618</td>\n",
       "      <td>260</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117061</th>\n",
       "      <td>3618</td>\n",
       "      <td>480</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23817</th>\n",
       "      <td>3618</td>\n",
       "      <td>1270</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133952</th>\n",
       "      <td>3618</td>\n",
       "      <td>1198</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57478</th>\n",
       "      <td>3618</td>\n",
       "      <td>608</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38419</th>\n",
       "      <td>3618</td>\n",
       "      <td>2762</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989744</th>\n",
       "      <td>3618</td>\n",
       "      <td>3434</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996266</th>\n",
       "      <td>3618</td>\n",
       "      <td>1829</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988059</th>\n",
       "      <td>3618</td>\n",
       "      <td>3805</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883688</th>\n",
       "      <td>3618</td>\n",
       "      <td>2537</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996322</th>\n",
       "      <td>3618</td>\n",
       "      <td>2758</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958568</th>\n",
       "      <td>3618</td>\n",
       "      <td>1114</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989387</th>\n",
       "      <td>3618</td>\n",
       "      <td>2849</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848443</th>\n",
       "      <td>3618</td>\n",
       "      <td>418</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997151</th>\n",
       "      <td>3618</td>\n",
       "      <td>3586</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993237</th>\n",
       "      <td>3618</td>\n",
       "      <td>2678</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959753</th>\n",
       "      <td>3618</td>\n",
       "      <td>659</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792126</th>\n",
       "      <td>3618</td>\n",
       "      <td>3464</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995893</th>\n",
       "      <td>3618</td>\n",
       "      <td>1549</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888684</th>\n",
       "      <td>3618</td>\n",
       "      <td>3465</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883721</th>\n",
       "      <td>3618</td>\n",
       "      <td>2415</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369424</th>\n",
       "      <td>3618</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324907</th>\n",
       "      <td>3618</td>\n",
       "      <td>968</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414142</th>\n",
       "      <td>3618</td>\n",
       "      <td>288</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466363</th>\n",
       "      <td>3618</td>\n",
       "      <td>2718</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541203</th>\n",
       "      <td>3618</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781108</th>\n",
       "      <td>3618</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822637</th>\n",
       "      <td>3618</td>\n",
       "      <td>2147</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605298</th>\n",
       "      <td>3618</td>\n",
       "      <td>2159</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474696</th>\n",
       "      <td>3618</td>\n",
       "      <td>839</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932297</th>\n",
       "      <td>3618</td>\n",
       "      <td>2170</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830397</th>\n",
       "      <td>3618</td>\n",
       "      <td>2314</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790260</th>\n",
       "      <td>3618</td>\n",
       "      <td>1816</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845232</th>\n",
       "      <td>3618</td>\n",
       "      <td>2435</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943601</th>\n",
       "      <td>3618</td>\n",
       "      <td>3459</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961146</th>\n",
       "      <td>3618</td>\n",
       "      <td>3221</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1344 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer_id  product_id  star_rating  user  item\n",
       "136535         3618         593            5     6     9\n",
       "190544         3618         858            5     6    24\n",
       "219754         3618          50            5     6    38\n",
       "600199         3618        1304            5     6    80\n",
       "340827         3618        1358            5     6   135\n",
       "505978         3618        2324            5     6   143\n",
       "453750         3618        1234            5     6   187\n",
       "16115          3618        1035            5     6   269\n",
       "612908         3618        1947            5     6   345\n",
       "591457         3618         899            5     6   352\n",
       "563120         3618         953            5     6   371\n",
       "695661         3618        1952            5     6   437\n",
       "2424           3618         914            5     6   459\n",
       "158429         3618        2501            5     6   510\n",
       "617517         3618         508            5     6   543\n",
       "152122         3618        1124            5     6   785\n",
       "162054         3618        1096            5     6   947\n",
       "902041         3618        3007            5     6  1074\n",
       "693795         3618        2875            5     6  1082\n",
       "831150         3618        1286            5     6  1100\n",
       "852019         3618        3194            5     6  1328\n",
       "13873          3618         938            5     6  1489\n",
       "999931         3618         572            5     6  3526\n",
       "107701         3618        2858            4     6     0\n",
       "47411          3618         260            4     6     1\n",
       "117061         3618         480            4     6     4\n",
       "23817          3618        1270            4     6     8\n",
       "133952         3618        1198            4     6    11\n",
       "57478          3618         608            4     6    12\n",
       "38419          3618        2762            4     6    13\n",
       "...             ...         ...          ...   ...   ...\n",
       "989744         3618        3434            2     6  2687\n",
       "996266         3618        1829            2     6  2717\n",
       "988059         3618        3805            2     6  2751\n",
       "883688         3618        2537            2     6  2757\n",
       "996322         3618        2758            2     6  2805\n",
       "958568         3618        1114            2     6  2816\n",
       "989387         3618        2849            2     6  2826\n",
       "848443         3618         418            2     6  2846\n",
       "997151         3618        3586            2     6  2877\n",
       "993237         3618        2678            2     6  2915\n",
       "959753         3618         659            2     6  2925\n",
       "792126         3618        3464            2     6  3024\n",
       "995893         3618        1549            2     6  3045\n",
       "888684         3618        3465            2     6  3068\n",
       "883721         3618        2415            2     6  3116\n",
       "369424         3618        2021            1     6   323\n",
       "324907         3618         968            1     6   383\n",
       "414142         3618         288            1     6   396\n",
       "466363         3618        2718            1     6   834\n",
       "541203         3618        2004            1     6   862\n",
       "781108         3618        1480            1     6  1168\n",
       "822637         3618        2147            1     6  1284\n",
       "605298         3618        2159            1     6  1332\n",
       "474696         3618         839            1     6  1786\n",
       "932297         3618        2170            1     6  1855\n",
       "830397         3618        2314            1     6  1976\n",
       "790260         3618        1816            1     6  2003\n",
       "845232         3618        2435            1     6  2024\n",
       "943601         3618        3459            1     6  2353\n",
       "961146         3618        3221            1     6  2854\n",
       "\n",
       "[1344 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df[reduced_df['user'] == 6].sort_values(['star_rating', 'item'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, user #6 seems to like sprawling dramamtic television series and sci-fi, but they dislike silly comedies.\n",
    "\n",
    "Now we'll loop through and predict user #6's ratings for every common video in the catalog, to see which ones we'd recommend and which ones we wouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                 'product_id': array.tolist()}))\n",
    "\n",
    "predictions = pd.DataFrame({'product_id': product_index['product_id'],\n",
    "                            'prediction': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our predicted highly rated shows have some well-reviewed TV dramas and some sci-fi.  Meanwhile, our bottom rated shows include goofball comedies.\n",
    "\n",
    "*Note, because of random initialization in the weights, results on subsequent runs may differ slightly.*\n",
    "\n",
    "Let's confirm that we no longer have almost perfect correlation in recommendations with user #7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_user7 = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions_user7 += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 7]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                       'product_id': array.tolist()}))\n",
    "plt.scatter(predictions['prediction'], np.array(predictions_user7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this example, we developed a deep learning model to predict customer ratings.  This could serve as the foundation of a recommender system in a variety of use cases.  However, there are many ways in which it could be improved.  For example we did very little with:\n",
    "- hyperparameter tuning\n",
    "- controlling for overfitting (early stopping, dropout, etc.)\n",
    "- testing whether binarizing our target variable would improve results\n",
    "- including other information sources (video genres, historical ratings, time of review)\n",
    "- adjusting our threshold for user and item inclusion \n",
    "\n",
    "In addition to improving the model, we could improve the engineering by:\n",
    "- Setting the context and key value store up for distributed training\n",
    "- Fine tuning our data ingestion (e.g. num_workers on our data iterators) to ensure we're fully utilizing our GPU\n",
    "- Thinking about how pre-processing would need to change as datasets scale beyond a single machine\n",
    "\n",
    "Beyond that, recommenders are a very active area of research and techniques from active learning, reinforcement learning, segmentation, ensembling, and more should be investigated to deliver well-rounded recommendations.\n",
    "\n",
    "### Clean-up (optional)\n",
    "\n",
    "Let's finish by deleting our endpoint to avoid stray hosting charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-mxnet-2018-12-10-02-52-40-970\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
